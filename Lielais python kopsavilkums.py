import nltk
import re
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
import langid
import string
import textdistance
from textblob import TextBlob
import langdetect
import random
from deep_translator import GoogleTranslator
from googletrans import Translator
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Uzdevums 1: VÄrdu bieÅ¾uma sadalÄ«jums tekstÄ
def task_1():
    text = "MÄkoÅ†ainÄ dienÄ kaÄ·is sÄ“dÄ“ja uz palodzes. KaÄ·is domÄja, kÄpÄ“c debesis ir pelÄ“kas. KaÄ·is gribÄ“ja redzÄ“t sauli, bet saule slÄ“pÄs aiz mÄkoÅ†iem."
    text = text.lower()
    words = word_tokenize(text)
    words = [word for word in words if word.isalnum()]
    freq_dist = FreqDist(words)

    for word, frequency in freq_dist.items():
        print(f"{word}: {frequency}")

# Uzdevums 2: Valodas noteikÅ¡ana vairÄkos tekstos
def task_2():
    texts = [
        "Å odien ir saulaina diena.",
        "Today is a sunny day.",
        "Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ.",
        "Å is ir mans teksts.", 
        "æˆ‘æœ‰ä¸€æ£µå¤§æ ‘ï¼Œæƒ³åƒçš„æ—¶å€™å°±åƒ.", 
        "Ğ¡Ğ°Ñ€Ğ° Ğ°ĞºĞ¾Ğ¼Ğ¿Ğ¸ÑƒÑ‚ĞµÑ€ laaye Ğ¸Ğ·Ò©Ñ‹Ñ€Ñ† Ğ¸ÑĞ°Ó¡ÑŒÑ‹ ÑÑ‹Ğ¼Ğ¾ÑƒĞ¿.", 
        "qoghDu'Daq meQchoH nuq 'oH mIwvam'e'?",
        "'eko fay tsawl Na'vi!"
    ]

    for text in texts:
        language, confidence = langid.classify(text)  
        print(f"\"{text}\" - valoda: {language} - precizitÄte: {confidence:.2f}")

# Uzdevums 3: VÄrdu sakritÄ«ba starp diviem tekstiem
def task_3():
    def calculate_word_overlap(text1, text2):
        translator = str.maketrans('', '', string.punctuation)
        text1_cleaned = text1.translate(translator).lower().split()
        text2_cleaned = text2.translate(translator).lower().split()
        similarity = textdistance.jaccard.similarity(set(text1_cleaned), set(text2_cleaned))
        overlap_percentage = similarity * 100
        common_words = set(text1_cleaned).intersection(set(text2_cleaned))
        return common_words, overlap_percentage

    text1 = "Rudens lapas ir dzeltenas un oranÅ¾as. Lapas klÄj zemi un padara to krÄsainu."
    text2 = "KrÄsainas rudens lapas krÄ«t zemÄ“. Lapas ir oranÅ¾as un dzeltenas."

    common_words, overlap_percentage = calculate_word_overlap(text1, text2)
    print(f"KopÄ«gie vÄrdi: {', '.join(common_words)}")
    print(f"SakritÄ«bas procentuÄlais lÄ«menis: {overlap_percentage:.2f}%")

# Uzdevums 4: NoskaÅ†ojuma analÄ«ze ar tulkoÅ¡anas atbalstu
def task_4():
    sentences = [
        "This is great!",
        "I am dissapointed, this is shit!",
        "NeitrÄls produkts, neko nejÅ«tu."
    ]

    def analyze_sentiment(sentence):
        try:
            lang = langdetect.detect(sentence)
        except Exception as e:
            print(f"Valodas noteikÅ¡ana neizdevÄs: {e}")
            return None
        if lang == 'en':
            analysis = TextBlob(str(sentence))
        else:
            translated = GoogleTranslator(source=lang, target='en').translate(sentence)
            blob = TextBlob(str(translated))
            analysis = blob
        
        polarity = analysis.sentiment.polarity
        if polarity > 0:
            return "pozitÄ«vs"
        elif polarity < 0:
            return "negatÄ«vs"
        else:
            return "neitrÄls"

    for sentence in sentences:
        try:
            lang = langdetect.detect(sentence)
        except Exception as e:
            print(f"Valodas noteikÅ¡ana neizdevÄs teikumam {sentence}: {e}")
            continue
        sentiment = analyze_sentiment(sentence)
        original_text = sentence
        detected_lang = lang
        if detected_lang != 'en':
            try:
                translated_back = translate_text(sentiment, 'en', detected_lang)
                analyzed_text = translated_back
            except Exception as e:
                print(f"TulkoÅ¡ana neizdevÄs: {e}")
                analyzed_text = sentiment
        else:
            analyzed_text = sentiment
        print(f"Teikums: \"{original_text}\" -> NoskaÅ†ojums: {analyzed_text}")

# Uzdevums 5: Teksta tÄ«rÄ«Å¡ana un normalizÄcija
def task_5():
    def clean_and_normalize_text(raw_text):
        text = re.sub(r'@[\w]+', '', raw_text)
        text = re.sub(r'http\S+', '', text)  
        text = re.sub(r'[^\w\s]', '', text) 
        text = text.lower()
        text = re.sub(r'\s+', ' ', text) 
        text = text.strip() 
        return text

    raw_text = "@John: Å is ir lielisks produkts!!! Vaine? ğŸ‘ğŸ‘ğŸ‘ http://example.com"
    cleaned_text = clean_and_normalize_text(raw_text)
    print(cleaned_text)

# Uzdevums 6: AutomÄtiska rezumÄ“Å¡ana
def summarize_article(text, sentences_count=2):
    translator = Translator()
    translated_text = translator.translate(text, src="lv", dest="en").text

    parser = PlaintextParser.from_string(translated_text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = [str(sentence) for sentence in summarizer(parser.document, sentences_count)]
    
    summary_text = " ".join(summary)
    back_translated = translator.translate(summary_text, src="en", dest="lv").text
    return back_translated
text = """Latvija ir valsts Baltijas reÄ£ionÄ. TÄs galvaspilsÄ“ta ir RÄ«ga, kas ir slavena ar savu vÄ“sturisko centru un skaistajÄm Ä“kÄm. Latvija robeÅ¾ojas ar Lietuvu, Igauniju un Krieviju, kÄ arÄ« tai ir piekÄ¼uve Baltijas jÅ«rai. TÄ ir viena no Eiropas SavienÄ«bas dalÄ«bvalstÄ«m."""
summary = summarize_article(text)
print("RezumÄ“jums:")
print(summary)

# Uzdevums 7
def task_7():
    print("Å¡obrÄ«d vÄ“l taisu")

# Uzdevums 8: Personu un organizÄciju atpazÄ«Å¡ana
def task_8():
    text = "Valsts prezidents Egils Levits piedalÄ«jÄs pasÄkumÄ, ko organizÄ“ja Latvijas UniversitÄte."
    tokens = nltk.word_tokenize(text)
    tags = nltk.pos_tag(tokens)
    entities = nltk.ne_chunk(tags)
    person_names = []
    organizations = []
    for subtree in entities:
        if isinstance(subtree, nltk.Tree):  
            label = subtree.label()
            if label == 'PERSON': 
                person_names.append(" ".join(word for word, tag in subtree))
            elif label == 'GPE' or label == 'LOCATION': 
                organizations.append(" ".join(word for word, tag in subtree))
    print("PersonvÄrdi:", person_names)
    print("OrganizÄcijas:", organizations)

# Uzdevums 9: StÄstu Ä£enerÄ“Å¡ana
def task_9():
    start = "Reiz kÄdÄ tÄlÄ zemÄ“"
    story = [start]
    story_parts = [
        "KÄds mazs zÄ“ns devÄs slÄ«pÄ“t.",
        "Bembis nopÄ¼Äva pus meÅ¾u.",
        "CilvÄ“ki dzÄ«voja mierÄ un harmonijÄ."
    ]
    for _ in range(1):
        next_part = random.choice(story_parts)
        story_parts.remove(next_part)  
        story.append(next_part)
    print(" ".join(story))

# Uzdevums 10: Tulkot tekstus
def task_10():
    texts = ["Labdien! KÄ jums klÄjas?", "Es Å¡odien lasÄ«ju interesantu grÄmatu."]
    translator = Translator()
    for text in texts:
        translation = translator.translate(text, src='lv', dest='en')
        print(f"Ievades teksts: {text}")
        print(f"Tulkots teksts: {translation.text}")

# IzvÄ“lne, lai izvÄ“lÄ“tos uzdevumu, kuru izpildÄ«t
def main():
    while True:
        print("\nIzvÄ“lies uzdevumu, kuru veikt:")
        print("1. VÄrdu bieÅ¾uma sadalÄ«jums")
        print("2. Valodas noteikÅ¡ana vairÄkos tekstos")
        print("3. VÄrdu sakritÄ«bas aprÄ“Ä·ins")
        print("4. NoskaÅ†ojuma analÄ«ze")
        print("5. Teksta tÄ«rÄ«Å¡ana un normalizÄcija")
        print("6. AutomÄtiska rezumÄ“Å¡ana")
        print("7. Å obrÄ«d vÄ“l taisu")
        print("8. Personu un organizÄciju atpazÄ«Å¡ana")
        print("9. StÄstu Ä£enerÄ“Å¡ana")
        print("10. Tulkot tekstus")
        print("0. Iziet")
        
        choice = input("Ievadiet uzdevuma numuru: ")

        if choice == '1':
            task_1()
        elif choice == '2':
            task_2()
        elif choice == '3':
            task_3()
        elif choice == '4':
            task_4()
        elif choice == '5':
            task_5()
        elif choice == '6':
            task_6()
        elif choice == '7':
            task_7()
        elif choice == '8':
            task_8()
        elif choice == '9':
            task_9()
        elif choice == '10':
            task_10()
        elif choice == '0':
            break
        else:
            print("Nepareiza izvÄ“le. LÅ«dzu, mÄ“Ä£iniet vÄ“lreiz.")

# IzpildÄ«t programmu
if __name__ == "__main__":
    main()
